// Data Deployer - Populates production database with SQL structure and data via Lambda
//
// Deploys:
// - Static structure (sefarim, perakim, dates)
// - Perushim structure and data (parshanim, perushim, notes â€” generated by Sefaria pipeline)
//
// Dynamic structure (articles, dedications, authors, etc.) is managed manually via admin zone.

import fs from "node:fs";
import path from "node:path";
import { fileURLToPath } from "node:url";
import { InvokeCommand, LambdaClient } from "@aws-sdk/client-lambda";
import {
	DeleteObjectsCommand,
	PutObjectCommand,
	S3Client,
} from "@aws-sdk/client-s3";
import * as dotenv from "dotenv";
import { DeployerBase } from "../deployer-base.mjs";

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Load environment variables
dotenv.config({
	path: "./deploy/data-deploy/.env",
});

// Configuration
const S3_BUCKET = "bible-on-site-data-deploy";
const LAMBDA_FUNCTION_NAME = "bible-on-site-db-populator";

// SQL files to deploy (order matters: structure before data)
const SQL_FILES = [
	"tanah_static_structure.sql",
	"tanah_sefarim_and_perakim_data.sql",
	"perushim_structure.sql",
	"perushim_data.sql",
];

class DataDeployer extends DeployerBase {
	private readonly region: string;
	private readonly dataDir: string;
	private readonly sqlFiles = SQL_FILES;
	private s3Prefix = "";

	constructor() {
		super("data");

		const region = process.env.AWS_REGION;

		if (!region) {
			throw new Error("AWS_REGION environment variable is required");
		}

		this.region = region;
		this.dataDir = path.resolve(__dirname, "../../../data/mysql");
	}

	protected override async deployPreConditions(): Promise<void> {
		this.info("Checking preconditions...");

		// Verify all SQL files exist
		for (const sqlFile of this.sqlFiles) {
			const filePath = path.join(this.dataDir, sqlFile);
			if (!fs.existsSync(filePath)) {
				throw new Error(`Required SQL file not found: ${filePath}`);
			}
		}

		this.info("Preconditions passed.");
	}

	protected override async coreDeploy(): Promise<void> {
		const s3Client = new S3Client({ region: this.region });
		const lambdaClient = new LambdaClient({ region: this.region });

		try {
			// Upload SQL files to S3
			await this.uploadSqlFiles(s3Client);

			// Invoke Lambda to populate database
			await this.invokeLambda(lambdaClient);
		} finally {
			// Cleanup S3 files
			await this.cleanupS3Files(s3Client);
		}
	}

	private async uploadSqlFiles(s3Client: S3Client): Promise<void> {
		const timestamp = new Date().toISOString().replace(/[:.]/g, "-");
		this.s3Prefix = `sql/${timestamp}/`;

		this.info(`Uploading SQL files to s3://${S3_BUCKET}/${this.s3Prefix}...`);

		for (const sqlFile of this.sqlFiles) {
			const filePath = path.join(this.dataDir, sqlFile);
			const content = fs.readFileSync(filePath);

			await s3Client.send(
				new PutObjectCommand({
					Bucket: S3_BUCKET,
					Key: `${this.s3Prefix}${sqlFile}`,
					Body: content,
					ContentType: "text/plain",
				}),
			);

			this.info(`Uploaded: ${sqlFile}`);
		}
	}

	private async invokeLambda(lambdaClient: LambdaClient): Promise<void> {
		this.info(`Invoking Lambda function: ${LAMBDA_FUNCTION_NAME}...`);

		const payload = {
			sql_files: this.sqlFiles,
			s3_prefix: this.s3Prefix,
		};

		const response = await lambdaClient.send(
			new InvokeCommand({
				FunctionName: LAMBDA_FUNCTION_NAME,
				Payload: Buffer.from(JSON.stringify(payload)),
				LogType: "Tail",
			}),
		);

		// Decode and log the Lambda logs
		if (response.LogResult) {
			const logs = Buffer.from(response.LogResult, "base64").toString("utf-8");
			this.info("Lambda logs:");
			console.log(logs);
		}

		// Check for errors
		if (response.FunctionError) {
			const errorPayload = response.Payload
				? JSON.parse(Buffer.from(response.Payload).toString("utf-8"))
				: {};
			throw new Error(
				`Lambda execution failed: ${errorPayload.errorMessage || response.FunctionError}`,
			);
		}

		// Parse response
		const result = response.Payload
			? JSON.parse(Buffer.from(response.Payload).toString("utf-8"))
			: {};

		this.info("Lambda response:");
		console.log(JSON.stringify(result, null, 2));

		if (result.statusCode !== 200) {
			throw new Error(
				`Lambda returned non-200 status: ${JSON.stringify(result)}`,
			);
		}

		this.info("Database population completed successfully.");
	}

	private async cleanupS3Files(s3Client: S3Client): Promise<void> {
		if (!this.s3Prefix) return;

		this.info(`Cleaning up S3 files at s3://${S3_BUCKET}/${this.s3Prefix}...`);

		try {
			await s3Client.send(
				new DeleteObjectsCommand({
					Bucket: S3_BUCKET,
					Delete: {
						Objects: this.sqlFiles.map((f) => ({
							Key: `${this.s3Prefix}${f}`,
						})),
					},
				}),
			);
			this.info("S3 cleanup completed.");
		} catch (error) {
			this.warn(`S3 cleanup failed (non-fatal): ${error}`);
		}
	}
}

// Main entry point
const deployer = new DataDeployer();
await deployer.deploy();
